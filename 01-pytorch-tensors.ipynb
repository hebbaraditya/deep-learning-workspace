{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-pytorch-tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PyTorch !!?\n",
    "\n",
    "According to their official website [https://pytorch.org/] `PyTorch` is An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
    "\n",
    "And when you move to their github page [https://github.com/pytorch/pytorch] you will find these points\n",
    "\n",
    "At the core `PyTorch` is a `python` package that provides two high-level features:\n",
    "* Tensor computation (like NumPy) with strong GPU acceleration\n",
    "* Deep neural networks built on a tape-based autograd system\n",
    "\n",
    "We can reuse our favorite python packages such as `NumPy`, `SciPy`, and `Cython` to extend `Pytorch` when needed\n",
    "\n",
    "Alright, too much theory!!\n",
    "Let's look at some code, later we can learn some more theory part or fundamentals\n",
    "\n",
    "`Note : Any of these are not own words, i crawl around websites and try \n",
    "to bring the best here ðŸ¤ª!! Sometimes it's harder to keep everything at one place; So I did this....[Ohho!!].`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "As I said earlier at it's core, Pytorch is a library for processing tensors. Instead of saying blah blah blah about tensor, in simple words `tensor is just collection of numbers in specific shape (really!?) which can run on both CPU and GPU.`\n",
    "\n",
    "or\n",
    "\n",
    "`In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor.`\n",
    "                                       - [link](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=as_li_ss_tl?ie=UTF8&qid=1513728680&sr=8-3&keywords=deep+learning&linkCode=sl1&tag=inspiredalgor-20&linkId=b1b8f643b74f61f5074ee6dc0c993de7)\n",
    "                                       \n",
    "or\n",
    "\n",
    "\n",
    "`A tensor is a number, vector, matrix or any n-dimensional array. `\n",
    "\n",
    "So let's create a tensor with a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing torch assuming that its already installed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a tensor with a single number\n",
    "t1 = torch.tensor(7)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking datatype of tensor t1\n",
    "t1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ummm can we create a tensor with float type !!??  YESSS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor(7.)\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. is just 7.0 don't worry!!. Let's check datatype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, until now we have seen pretty simple tensors, lets make some more with more complexity..... [Oh, No..!!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector tensor\n",
    "t3 = torch.tensor([1., 2., 3., 4.])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix array maybe(?) 2-dimensional\n",
    "t4 = torch.tensor([[5., 6],\n",
    "                   [7., 8.],\n",
    "                   [9., 10]])\n",
    "t4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3-dimensional array\n",
    "t5 =  torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6]\n",
    "     ]\n",
    "   ])\n",
    "t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the above example what its saying is a tensor with <b>3</b> layers/ depths with 2 row with 3 items. In simple words `3 two-dimensional arrays`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "When training neural network, as you might have heard the most frequently used algorithm is `back propagation`[really?]. The `back propagation algorithm` really requires gradient of the loss function[more about the later [but why!??ðŸ˜­]] \n",
    "\n",
    "\n",
    "Let's look at an example\n",
    "\n",
    "PS: It's not my pure example I am putting here; Its from PyTorch official website. Credits are given in the end  ðŸ¤·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.), tensor(8., requires_grad=True), tensor(9., requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating tensor\n",
    "x = torch.tensor(7.)\n",
    "w = torch.tensor(8., requires_grad=True)\n",
    "b = torch.tensor(9., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did was a simple arithmetic operation with the tensors that we created. It's somewhat related to ML/DL field. We can learn about it later. For now let's just see it as an arithmetic operation.\n",
    "Here with the help of PyTorch we can automatically compute derivative of `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivaties\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(7.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#Displaying gradients\n",
    "print(x.grad)\n",
    "#dy/dw\n",
    "print(w.grad)\n",
    "#dy/db\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `grad` is just gradient which just another word for derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more theory about PyTorch\n",
    "\n",
    "\n",
    "ðŸ¤ª\n",
    "\n",
    "Let's continue with some theory part\n",
    "In `PyTorch` we can reuse our favorite Python packages such as `NumPy`, `SciPy` and `Cython` to extend PyTorch when we needed\n",
    "\n",
    "\n",
    "To tell more about PyTorch :\n",
    "\n",
    "* A GPU- Ready Tensor Library\n",
    "* Dynamic Neural Networks: Tape-Based Autograd\n",
    "* Python First\n",
    "* Imperative Experiences\n",
    "* Fast and Lean\n",
    "* Extension without pain\n",
    "\n",
    "## At granular level, PyTorch is a library that consists of the following components:\n",
    "\n",
    "\n",
    "\n",
    "* <b>[torch](https://pytorch.org/docs/stable/torch.html)</b> :\tA Tensor library like NumPy, with strong GPU support\n",
    "* <b>[torch.autograd](https://pytorch.org/docs/stable/autograd.html) </b>:\tA tape-based automatic differentiation library that supports all differentiable Tensor operations in torch\n",
    "* <b>[torch.jit](https://pytorch.org/docs/stable/jit.html)</b>:\tA compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code\n",
    "* <b>[torch.nn](https://pytorch.org/docs/stable/nn.html)</b>:\tA neural networks library deeply integrated with autograd designed for maximum flexibility\n",
    "* <b>[torch.multiprocessing](https://pytorch.org/docs/stable/multiprocessing.html)</b>:\tPython multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training\n",
    "* <b>[torch.utils](https://pytorch.org/docs/stable/data.html)</b>:\tDataLoader and other utility functions for convenience\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and credits\n",
    "\n",
    "This is just an introduction that I have done here. More things on PyTorch will be coming soon. With all respect i am giving credits for the authors or notebooks that I have referred for this notebook/blog.\n",
    "\n",
    "* https://github.com/pytorch/pytorch\n",
    "* https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be\n",
    "* https://pytorch.org/\n",
    "* https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans/lesson/lesson-1-pytorch-basics-and-linear-regression\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
